{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama"
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ],
      "cuda-detection": false
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/dqd09xa1gr1dks6vw7snan3np9am1wnk-ollama-0.6.7.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "ollama-0.6.7",
      "pname": "ollama",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:21:37.226770Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.6.7",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/6ixsppwv88q4qj0ymnx4ayyjm20bnf29-ollama-0.6.7"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/s6q511f3gv16i3l7iq4p2hwr9hz943k5-ollama-0.6.7.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "ollama-0.6.7",
      "pname": "ollama",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:40:25.779536Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.6.7",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/cgbb8lrv802qczyqja9y6bjv9jnn9y62-ollama-0.6.7"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/azx476rzwpqhq32cnkcf65m6m1mj8nkg-ollama-0.6.7.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "ollama-0.6.7",
      "pname": "ollama",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T05:18:52.496363Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.6.7",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/kgvxldmw5c9braw1ix5k6xv23yp2zm2n-ollama-0.6.7"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/bdkgvd2kyfydcmkzlbhs86sbp0rh6v0x-ollama-0.6.7.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "ollama-0.6.7",
      "pname": "ollama",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:56:48.194763Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.6.7",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/iv9w5dwxjpklxxkizwksi9vvfkwvbg9k-ollama-0.6.7"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/3ynhm9d9sgkhbigsjr19g4hlglgr2aap-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:21:36.421891Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/kr95cf4q9w7xahyfgx3b1zcnpg7s1xya-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/b9zjka3y3gww8050h7jyyhfk0lq2sprx-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:40:24.247845Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/lpbk1wg31nl50wqr1rrycilkwyxj55mc-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/vf53dzyj15680spry9yaxgzx55m4ai6x-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T05:18:50.781087Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/7h4lp9zz6b7zbj87hhg87mw0xarqci5q-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/pi8k3gfdm3qvmk5dglpgwvwrm8yycnv4-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "adaa24fbf46737f3f1b5497bf64bae750f82942e",
      "rev_count": 799423,
      "rev_date": "2025-05-13T23:30:50Z",
      "scrape_date": "2025-05-16T04:56:47.412193Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/lfmfn02cqxivs57n3cq0yabjrh0payvz-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    }
  ]
}
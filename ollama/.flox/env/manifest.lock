{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama",
        "systems": [
          "x86_64-darwin",
          "aarch64-darwin"
        ]
      },
      "ollama-cuda": {
        "pkg-path": "flox/ollama-cuda",
        "priority": 6,
        "systems": [
          "x86_64-linux",
          "aarch64-linux"
        ]
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ]
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/dhgr2z66qpjv79ni5ghjz262nj45xyxy-ollama-0.12.6.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "ollama-0.12.6",
      "pname": "ollama",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-10-31T03:32:22.112711Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.6",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/qmj8pj0xc62ch9km72i9yhzrkn677jsd-ollama-0.12.6"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/zmban09zf8riiwmyixi1k526yz1spsgp-ollama-0.12.6.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "ollama-0.12.6",
      "pname": "ollama",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-11-05T16:45:43.130757Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.6",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/a0h583zbxl2kpzc1rnpjgxhw3d94khhc-ollama-0.12.6"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/8whq14v5zgjxz45fiyrnyxlfv7s1j4id-ollama-0.12.6.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "https://github.com/barstoolbluz/ollama-cuda?rev=ddfec7057b493804cc9e3bdfd8452638f496c551",
      "name": "ollama-0.12.6",
      "pname": "ollama",
      "rev": "ddfec7057b493804cc9e3bdfd8452638f496c551",
      "rev_count": 17,
      "rev_date": "2025-11-06T02:11:10Z",
      "scrape_date": "2025-11-19T11:06:29.940687996Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.6",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/c9757rcmaa3w1xrks2yrywf919w3pyj2-ollama-0.12.6"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/aafbqbjbfy2nyr8k0i03rxgcj57kf8pa-ollama-0.12.6.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "https://github.com/barstoolbluz/ollama-cuda?rev=ddfec7057b493804cc9e3bdfd8452638f496c551",
      "name": "ollama-0.12.6",
      "pname": "ollama",
      "rev": "ddfec7057b493804cc9e3bdfd8452638f496c551",
      "rev_count": 17,
      "rev_date": "2025-11-06T02:11:10Z",
      "scrape_date": "2025-11-19T11:06:29.940689138Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.6",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/grr9x96avfn4816j9qadwqda6i7d6dj3-ollama-0.12.6"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/sfj8fn1pp10418sk9i28xlhwlh6ic5va-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-10-31T03:32:18.490393Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/hr78nmhc8pk0ab0280i7srzs4wzbfsl6-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/zvis3bzvivhws0kmxdqyxabi14skmvja-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-11-05T16:19:09.101132Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/nwx96z0v6amkhdrrxz1lrdymsyj20grf-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/w5x6f3m632rrzws6ninzknd6n8g9yacq-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-11-05T16:45:39.256835Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/k5scpmhlfwfby9wwpznhgbzhm0k8y8hz-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/9swy4syf6z6i5ymvk9r80qji3xm58zlc-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "08dacfca559e1d7da38f3cf05f1f45ee9bfd213c",
      "rev_count": 886100,
      "rev_date": "2025-10-28T17:26:24Z",
      "scrape_date": "2025-11-05T17:10:38.684870Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/b01flkwv4w3zc8gg4jgw52189iq7l9h3-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}

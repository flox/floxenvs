{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama"
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ],
      "cuda-detection": false
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/mdw020yz2gw448jqik81c29w0yc8l54q-ollama-0.12.9.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "ollama-0.12.9",
      "pname": "ollama",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T16:41:41.122063Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.9",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/3khvcwrv2dplxlnwpd9i58w0f85dy85k-ollama-0.12.9"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/jsgvz3s82rn3nbbrf19pz5wq0wx4q444-ollama-0.12.9.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "ollama-0.12.9",
      "pname": "ollama",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T17:19:05.718515Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.9",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/kn6klcazg5d5qwlq9r495w17mymc8anl-ollama-0.12.9"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/fg268g3v38s0cvw8nwjb7fdbk97n9061-ollama-0.12.9.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "ollama-0.12.9",
      "pname": "ollama",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T17:52:41.809490Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.9",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/939fjvdkmn77bpz7kndfrpiqxrn1bbsz-ollama-0.12.9"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/kryfmmrin51qm7w6407d1hhrmgnva3ip-ollama-0.12.9.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "ollama-0.12.9",
      "pname": "ollama",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T18:32:20.461350Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.12.9",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/ddpli05pa7n76nvlsw8fc44cwac53azm-ollama-0.12.9"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/8q9j1gq4wpf46d0z2ha1adna7xa6c6j5-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T16:41:37.451461Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/d0x8z3qhpq5fmsq7jw513rkpfdaiccbz-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/53zz5f0qp6bw4aw68r2anmg72ck2144b-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T17:19:00.596136Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/8dpldz7zmz6h4ddqzb0vfc47qwczhqyc-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/zsmsj0jgxvsckdcyscf6lssh9l7aj7xn-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T17:52:38.122380Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/z1bz9p0yk6lqhcd9ivvh4cmcrnbpyj5g-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/i88bj6z391q7qh3qhq7zqrp2hfc31m7x-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "c5ae371f1a6a7fd27823bc500d9390b38c05fa55",
      "rev_count": 895122,
      "rev_date": "2025-11-12T20:02:36Z",
      "scrape_date": "2025-11-14T18:32:15.050294Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/mh9lhwid2gci0s5x6s7z24gqx30hx2sk-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}

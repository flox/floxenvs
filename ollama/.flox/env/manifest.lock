{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama",
        "systems": [
          "x86_64-darwin",
          "aarch64-darwin"
        ]
      },
      "ollama-cuda": {
        "pkg-path": "flox/ollama-cuda",
        "priority": 6,
        "systems": [
          "x86_64-linux",
          "aarch64-linux"
        ]
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ]
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/x27k9y85l5fq78xx12980xchaxjrrcb0-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:02:55.596055Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/k02j91b7s8kpk8ipswnn2df7vg9922va-ollama-0.13.5"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/7rfvigz22f2g5zz9y5mf7sq35m760lr0-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:35:21.942539Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/9v7gynmmx0rnmnyh6q2lf16xpxsnv32v-ollama-0.13.5"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/s5aq24kq4s4lyq2pcpavfh28nhrvp3qc-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "https://github.com/barstoolbluz/build-ollama-cuda.git?rev=d725ecaa96be1bcf0a6eec4a611da932a7b99fb7",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "d725ecaa96be1bcf0a6eec4a611da932a7b99fb7",
      "rev_count": 24,
      "rev_date": "2025-12-30T14:39:17Z",
      "scrape_date": "2026-01-19T00:47:32.932344873Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/kckf7x832pr08wkmw0is86hj0vcw9s8x-ollama-0.13.5"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/fy9zi7fz65345csb18bnvkhxzff1dfvj-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "https://github.com/barstoolbluz/build-ollama-cuda?rev=d725ecaa96be1bcf0a6eec4a611da932a7b99fb7",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "d725ecaa96be1bcf0a6eec4a611da932a7b99fb7",
      "rev_count": 24,
      "rev_date": "2025-12-30T14:39:17Z",
      "scrape_date": "2026-01-19T00:47:32.932345714Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/vxyr0dpiaw9pnl182a8zl0kikh33f5j1-ollama-0.13.5"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/31vj18vaablraapb3jgmdnsli2ylvp1b-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:02:51.879227Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/kr3y5i5dkyn7wz602q6x2l1y3ylmd7ys-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/wycb4ks4kzi17fy7pzpcnwaljcdc3278-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:19:26.119683Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/q722zk1s8cfhj5rl4y2115ac0n3rd6hn-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/bgwib371vk8pq7pdwxbk7dvvfdrz8lyh-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:35:18.402066Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/ghh3kj36lwsgh2qwqi3rsqbx0zf65855-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/ggy7v32xzddsi9c8mx3cc5rwc3snxx14-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "3e2499d5539c16d0d173ba53552a4ff8547f4539",
      "rev_count": 916364,
      "rev_date": "2025-12-25T08:32:45Z",
      "scrape_date": "2025-12-26T03:51:44.509323Z",
      "stabilities": [
        "stable",
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/bx49d0bjrwbplkwq7jjjfabzvm1rkbrk-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}

{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama"
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ],
      "cuda-detection": false
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/0giij47rdrjr3jg26rphrbjaixwcf5zw-ollama-0.11.4.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "ollama-0.11.4",
      "pname": "ollama",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:27:14.738393Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.4",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/z6i9cialjqvdvfac6sx63bwpryj3mma3-ollama-0.11.4"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/wsm9y0dv489l43nq67rzyi58p9fnq329-ollama-0.11.4.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "ollama-0.11.4",
      "pname": "ollama",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:35:25.780379Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.4",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/05r5w9s5pdn1j05771hiijafmmn8bi0d-ollama-0.11.4"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/lf8gxlinml3fn1w9cw72iphcp9z52694-ollama-0.11.4.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "ollama-0.11.4",
      "pname": "ollama",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:43:54.672341Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.4",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/j969n66ybn52vsay9300h96021jy9jvl-ollama-0.11.4"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/1xn3ijp1wi2f6dli0xix604m1yhrb2fb-ollama-0.11.4.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "ollama-0.11.4",
      "pname": "ollama",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T02:12:18.522722Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.4",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/f1c4ix1zbmgpl007g86mwll3b52q328m-ollama-0.11.4"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/qh7vgc8r5cbsn33ij1y2xbxa7c7g4gxz-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:27:13.619786Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/7hrdjji8lh6m6lb2z6lvg3qhdnhgfxcg-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/q9snh23xf8gyd7na3rz51n30iqhszl5x-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:35:23.560556Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/5kfck03x88rdqvr07q4c672mdcnxkhj0-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/xif2ac4iwd5p61adf8c21idv4l1kkfd0-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T01:43:53.557468Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/f1ka7fqdpvics5mb9hwmwajxyrlbdsvw-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/wczx2s5wdchfwdpg2cna0svkmn9szjbg-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "fbcf476f790d8a217c3eab4e12033dc4a0f6d23c",
      "rev_count": 844474,
      "rev_date": "2025-08-14T15:51:38Z",
      "scrape_date": "2025-08-16T02:12:16.016566Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/asgjjjxidy0fby47rf4fqg1hd50gcrdg-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}
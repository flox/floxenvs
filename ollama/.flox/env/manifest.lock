{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama",
        "systems": [
          "x86_64-darwin",
          "aarch64-darwin"
        ]
      },
      "ollama-cuda": {
        "pkg-path": "flox/ollama-cuda",
        "priority": 6,
        "systems": [
          "x86_64-linux",
          "aarch64-linux"
        ]
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ]
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/r75xzxj4r0fr02h46bxxnhjz0mabbin6-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:03:18.190150Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/mh187d5w6p1sr3a23idrj45w146jk0gi-ollama-0.13.5"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/87i3877hrfc6dnq0plrb127dbzax21d5-ollama-0.13.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "ollama-0.13.5",
      "pname": "ollama",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:35:15.041269Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.13.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/9b6jvqq8zgrpvlr34wk38i0asw2b1fg1-ollama-0.13.5"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/pa70v5f8w6w4l7zqr7n5380ansxz912i-ollama-0.15.2.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "https://github.com/barstoolbluz/build-ollama-cuda.git?rev=ebc31330bfa8b9655563a11eed25ece4e5a8de54",
      "name": "ollama-0.15.2",
      "pname": "ollama",
      "rev": "ebc31330bfa8b9655563a11eed25ece4e5a8de54",
      "rev_count": 26,
      "rev_date": "2026-01-28T23:33:10Z",
      "scrape_date": "2026-02-02T00:54:32.076500785Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.15.2",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/zcbnz47lkaddy1syjk0igm2czacnaskd-ollama-0.15.2"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "ollama-cuda",
      "broken": false,
      "derivation": "/nix/store/3vl286v884w2gmrr533m2ag0s0zhp8jv-ollama-0.14.3.drv",
      "description": "Get up and running with large language models locally, using CUDA for NVIDIA GPU acceleration",
      "install_id": "ollama-cuda",
      "license": "MIT",
      "locked_url": "git@github.com:barstoolbluz/build-ollama-cuda?rev=0774feccaa8bb8d44e968e25ea40171cee7b5756",
      "name": "ollama-0.14.3",
      "pname": "ollama",
      "rev": "0774feccaa8bb8d44e968e25ea40171cee7b5756",
      "rev_count": 26,
      "rev_date": "2026-01-29T16:59:40Z",
      "scrape_date": "2026-02-02T00:54:32.076501908Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.14.3",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/prnfhsbhxwn5vb0x62931f7xx2fzxam9-ollama-0.14.3"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 6
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/w23q96rhsq79iq8ybzg5kakspcznlxfj-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:03:14.577051Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/wlc0w8xsqlf17bfsv8phx4pkjmv30jz8-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/aw2p8n9lly3ai21qxw1aripwv9zxrk49-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:19:24.972667Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/x7dv367h7f14yfvx3zk3jxs0p2pi336g-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/8b989h67m1hz91zq4q3z2w0mx98kzwnd-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:35:11.253379Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/sxl8w8xnz504hyfm9szgnc2pc9cwk7yv-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/w7bj7dpi6ns7dd5nrbzal9yj86qgj313-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=cad22e7d996aea55ecab064e84834289143e44a0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "cad22e7d996aea55ecab064e84834289143e44a0",
      "rev_count": 919991,
      "rev_date": "2025-12-30T17:40:09Z",
      "scrape_date": "2026-01-02T03:51:22.504850Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/qdbr62zzf6588myd3gi4ckp0zpax6ls3-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}

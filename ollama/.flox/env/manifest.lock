{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama"
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ],
      "cuda-detection": false
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/xyg3gv2iz5hf1265j8givzni3qhalck1-ollama-0.11.10.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "ollama-0.11.10",
      "pname": "ollama",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T02:06:37.348937Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.10",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/hclvgzyvbhm1gk4bf9gghi3s23jwkb6g-ollama-0.11.10"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/l2dkw3i8ax21zzdjd48x87k0j0xjgibl-ollama-0.11.10.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "ollama-0.11.10",
      "pname": "ollama",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T02:41:34.919002Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.10",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/bvvz1m2b6yc4wd7m6cfxpzgagrlbwx90-ollama-0.11.10"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/xask5fd8k5n3k4sw7l4hbhgh038rx1np-ollama-0.11.10.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "ollama-0.11.10",
      "pname": "ollama",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T03:13:40.690007Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.10",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/7anhnm0w3ak6n9v9malh57cbmq8dv1f6-ollama-0.11.10"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/z5r4bw6m8lsrx7mi8l2xrgfq85q2b2i3-ollama-0.11.10.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "ollama-0.11.10",
      "pname": "ollama",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T03:50:20.460839Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.11.10",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/qb4f38rxwhfkfvnwz45nsrhqavjij4f9-ollama-0.11.10"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/pkmafayb2frh3ww5cx4lb5jwwkxa2rk8-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T02:06:33.544746Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/6r4zj9ig6cz70wmvnbjmvp38j3mgg2vm-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/a9xz85zbz348c75bjvdnd3q1nynbndny-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T02:41:29.514811Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/5xy7c4c3yjglzbs5qh14xdigsa7yp99v-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/6b50hfbsywisfr4in0ih7g25lfbq45vq-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T03:13:36.994606Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/myldlmhg2kmqlsf4ivd8pnj7l8kw9z4r-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/d74rk5iknhk4xl4kbw9qbn16xj84dd34-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "ab0f3607a6c7486ea22229b92ed2d355f1482ee0",
      "rev_count": 859555,
      "rev_date": "2025-09-10T06:58:08Z",
      "scrape_date": "2025-09-12T03:50:14.921251Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/lznby8w001pykqp025q3jz97vv9drail-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}
{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "ollama": {
        "pkg-path": "ollama"
      },
      "ollama-ui": {
        "pkg-path": "nextjs-ollama-llm-ui"
      }
    },
    "vars": {
      "NEXT_PUBLIC_OLLAMA_URL": "http://localhost:11434"
    },
    "profile": {
      "common": "  if ollama list >/dev/null 2>&1; then\n    echo \"ü§ñ Ollama service running\"\n    echo \"üåê Web interface running on port 3000\"\n  else\n    echo \"‚õîÔ∏è Ollama service not available\"\n  fi\n"
    },
    "options": {
      "systems": [
        "aarch64-darwin",
        "aarch64-linux",
        "x86_64-linux",
        "x86_64-darwin"
      ],
      "cuda-detection": false
    },
    "services": {
      "ollama": {
        "command": "ollama serve"
      },
      "ollama-ui": {
        "command": "# wait for ollama to be ready\nuntil ollama list; do sleep 1; done\n\nexport NEXT_CACHE_DIR=\"$FLOX_ENV_CACHE/next\"\nmkdir -p $NEXT_CACHE_DIR\nnextjs-ollama-llm-ui\n"
      }
    }
  },
  "packages": [
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/9f973vmwp6w1g58nkavml3sl4ym68033-ollama-0.9.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "ollama-0.9.5",
      "pname": "ollama",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:14:34.245610Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.9.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/4kl089sr5wvbr9lhg6vqv0zrhdj32pmh-ollama-0.9.5"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/069jp2wm0agwh5g8np6z6qba20f8j5i9-ollama-0.9.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "ollama-0.9.5",
      "pname": "ollama",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:35:01.984178Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.9.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/v46nv34gs88kkwpisn7a19mwnbsj7gkv-ollama-0.9.5"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/a1gjbgama3imsvr2mv494w0vv8h76nxa-ollama-0.9.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "ollama-0.9.5",
      "pname": "ollama",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:53:52.639307Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.9.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/dbf5g8z0ksdv07m4m2j2axx0arcp8xrg-ollama-0.9.5"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "ollama",
      "broken": false,
      "derivation": "/nix/store/nhjpdhfzcfnhv68vyg63khjag09i0f9x-ollama-0.9.5.drv",
      "description": "Get up and running with large language models locally",
      "install_id": "ollama",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "ollama-0.9.5",
      "pname": "ollama",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T06:15:38.608366Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "0.9.5",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/l948c82zjjlqhjp5lwg9gbb13haf8kdz-ollama-0.9.5"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/aks094p6991rbann0nna3cz6z0zxbpm1-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:14:33.378438Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/3ykp13qarhkraxz9bkk7arrmmrn9zali-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/xl1hyas08iaqfj8z5cny6pccdsjp0aw9-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:35:00.331128Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/gv4pas85frdii40bcak254zfxsshd158-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "aarch64-linux",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/nxy0575ap51hj1n31ihyn3c8ifbycvpi-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T05:53:51.755950Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/3izbi3q4426g2b9q1zhx0ifv7g4dw7x9-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "nextjs-ollama-llm-ui",
      "broken": false,
      "derivation": "/nix/store/y79mhs2ixfgsl8sjmvbggdfyxv9ahkp4-nextjs-ollama-llm-ui-1.2.0.drv",
      "description": "Simple chat web interface for Ollama LLMs",
      "install_id": "ollama-ui",
      "license": "MIT",
      "locked_url": "https://github.com/flox/nixpkgs?rev=9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "name": "nextjs-ollama-llm-ui-1.2.0",
      "pname": "nextjs-ollama-llm-ui",
      "rev": "9807714d6944a957c2e036f84b0ff8caf9930bc0",
      "rev_count": 826938,
      "rev_date": "2025-07-08T14:16:20Z",
      "scrape_date": "2025-07-10T06:15:36.711251Z",
      "stabilities": [
        "staging",
        "unstable"
      ],
      "unfree": false,
      "version": "1.2.0",
      "outputs_to_install": [
        "out"
      ],
      "outputs": {
        "out": "/nix/store/xnw69lw0k8gd2mz7v284hyn4dbarz1vx-nextjs-ollama-llm-ui-1.2.0"
      },
      "system": "x86_64-linux",
      "group": "toplevel",
      "priority": 5
    }
  ]
}